{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "521467e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold will have: \n",
      "Total sample points: 4382, \n",
      "Training points: 3067 \n",
      "Validation points: 657,  \n",
      "Testing points: 658,  \n",
      "Stepping size: 658\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "notebook_dir = Path().resolve()\n",
    "src_path = notebook_dir.parent / 'src'\n",
    "sys.path.insert(0, str(src_path))\n",
    "import pandas as pd\n",
    "from utils.timesplit import RollingSplit\n",
    "\n",
    "crypto = pd.read_csv(\"../data/processed_data/BTCUSDT_1h_processed.csv\")\n",
    "\n",
    "# Variables\n",
    "input_features = ['open',\n",
    "                  'high', \n",
    "                  'low', \n",
    "                  'close']\n",
    "train_size = 0.7\n",
    "val_size = 0.15\n",
    "\n",
    "# Features and target\n",
    "X = crypto[input_features]\n",
    "y = crypto['log_return']\n",
    "\n",
    "# Initialize Rolling Splitter\n",
    "population = len(crypto)\n",
    "sample = round(population * 0.1)                # number of points within each fold\n",
    "train_count = int(train_size * sample)          # number of samples in the training set\n",
    "val_count = int(val_size * sample)              # number of samples in the validation set\n",
    "test_count = sample - train_count - val_count   # number of samples in the test set\n",
    "step_size = test_count                          # step size as test arbitrarily chosen as test size for non-overlapping sets\n",
    "\n",
    "splitter = RollingSplit(train_size=train_count, val_size=val_count, test_size=test_count, step_size=step_size)\n",
    "\n",
    "print(f\"Each fold will have: \\nTotal sample points: {sample}, \\nTraining points: {train_count} \\nValidation points: {val_count},  \\nTesting points: {test_count},  \\nStepping size: {step_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def random_seed(seed):\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def huber_loss(true, pred, delta=delta):\n",
    "    error = true - pred\n",
    "    abs_error = np.abs(error)\n",
    "    quadratic = np.minimum(abs_error, delta)\n",
    "    linear = abs_error - quadratic\n",
    "    loss = 0.5 * quadratic**2 + delta * linear\n",
    "\n",
    "    return np.mean(loss)\n",
    "\n",
    "# -> Optuna objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    \n",
    "    # => Param grid\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'delta': trial.suggest_float('delta', 0.5, 2.0)\n",
    "    }\n",
    "\n",
    "    # => MSE for each fold (maybe add huber loss later)\n",
    "    fold_loss = [] \n",
    "\n",
    "    for fold, (train_idx, val_idx, test_idx) in enumerate(splitter.split(X)):\n",
    "        \"\"\"\n",
    "        Instead of splitting data, we split the indices and use .iloc to get the data\n",
    "        \"\"\"\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val,   y_val   = X.iloc[val_idx],   y.iloc[val_idx]\n",
    "        \n",
    "        # => Model training\n",
    "        XGB_reg = XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            **params,\n",
    "            device = 'cuda'\n",
    "        )\n",
    "\n",
    "        # => Fitting the XGB model\n",
    "        XGB_reg.fit(X_train, y_train,\n",
    "                    eval_set = [(X_val, y_val)],\n",
    "                    early_stopping_rounds = 10,\n",
    "                    verbose = True)\n",
    "\n",
    "        XGB_reg_val_preds = XGB_reg.predict(X_val)\n",
    "        val_loss = mean_squared_error(y_val, XGB_reg_val_preds)\n",
    "        fold_loss.append(val_loss)\n",
    "\n",
    "    return np.mean(fold_loss)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=1, show_progress_bar=True)\n",
    "\n",
    "print(\"Best Parameters: \", study.best_params)\n",
    "print(\"Best Loss: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb9aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fold",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "val_mse",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_mse",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test r2",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3a1a7eb0-5555-42a7-a341-19cb72596789",
       "rows": [
        [
         "0",
         "1",
         "1.2796030112880714e-05",
         "2.100435847805939e-06",
         "0.8833421542314919"
        ],
        [
         "1",
         "2",
         "2.2928608685195535e-06",
         "8.287353071599428e-06",
         "0.4623350696554209"
        ],
        [
         "2",
         "3",
         "1.0160113656089803e-05",
         "4.147169206914812e-05",
         "-0.43267872914948224"
        ],
        [
         "3",
         "4",
         "4.132648834444768e-05",
         "2.699127724941368e-05",
         "0.07187847915105228"
        ],
        [
         "4",
         "5",
         "2.721033108515636e-05",
         "3.7110839927411978e-06",
         "0.7021178895862452"
        ],
        [
         "5",
         "6",
         "3.5607407603971065e-06",
         "3.5465694900893663e-05",
         "-0.018483369018777562"
        ],
        [
         "6",
         "7",
         "3.473629826549229e-05",
         "5.7813628012782474e-05",
         "-0.00985730706291621"
        ],
        [
         "7",
         "8",
         "5.7919460165655735e-05",
         "0.00010932207233871528",
         "-0.005543119723692591"
        ],
        [
         "8",
         "9",
         "0.00010921454820873038",
         "0.00011085138535970938",
         "0.37503843941641946"
        ],
        [
         "9",
         "10",
         "0.00010487698646503646",
         "0.00011841964080381677",
         "0.07003989579660963"
        ],
        [
         "10",
         "11",
         "0.00011750096996645694",
         "3.61040032883456e-05",
         "0.3742219424534149"
        ],
        [
         "11",
         "12",
         "3.294632490389547e-05",
         "1.5060341123811744e-05",
         "0.7493486228989041"
        ],
        [
         "12",
         "13",
         "1.4415649709193265e-05",
         "4.036637572053945e-05",
         "0.780810576351251"
        ],
        [
         "13",
         "14",
         "3.6494399957557775e-05",
         "8.966810765626389e-06",
         "0.919826108575983"
        ],
        [
         "14",
         "15",
         "2.1640544108411623e-05",
         "1.1167473728747985e-05",
         "0.7919161143776304"
        ],
        [
         "15",
         "16",
         "2.9839046954673914e-05",
         "2.645762088289942e-05",
         "0.552602671604399"
        ],
        [
         "16",
         "17",
         "3.229249760972461e-05",
         "2.2233392761700596e-05",
         "0.5133397336275014"
        ],
        [
         "17",
         "18",
         "1.3285480291230378e-05",
         "1.564591576282052e-05",
         "0.714036345133001"
        ],
        [
         "18",
         "19",
         "2.044709478083548e-05",
         "3.5739969161392664e-05",
         "0.09266863105534728"
        ],
        [
         "19",
         "20",
         "3.785286830177642e-05",
         "1.9807048127606076e-05",
         "0.6091587708567008"
        ],
        [
         "20",
         "21",
         "1.9552554084612222e-05",
         "6.5692574360793315e-06",
         "0.8356792898438715"
        ],
        [
         "21",
         "22",
         "6.464087125147442e-06",
         "3.4060576824431e-05",
         "0.4119051916303309"
        ],
        [
         "22",
         "23",
         "5.04467492443398e-05",
         "2.0359171806956553e-05",
         "0.5942531240428708"
        ],
        [
         "23",
         "24",
         "2.1637485194060123e-05",
         "1.2706566995546085e-05",
         "0.7605761542269351"
        ],
        [
         "24",
         "25",
         "1.4051541044255228e-05",
         "5.854746658679633e-06",
         "0.7540366292591218"
        ],
        [
         "25",
         "26",
         "6.171694126481685e-06",
         "6.699467083463036e-05",
         "0.1819359741750396"
        ],
        [
         "26",
         "27",
         "6.67974632201668e-05",
         "4.203424054226923e-05",
         "0.34442518946764145"
        ],
        [
         "27",
         "28",
         "3.972902608703119e-05",
         "7.441082940510176e-05",
         "0.04746037500436273"
        ],
        [
         "28",
         "29",
         "0.0001316717878763093",
         "4.2150929924900655e-05",
         "0.14047448790994344"
        ],
        [
         "29",
         "30",
         "4.9992220619026105e-05",
         "1.2754983689346054e-05",
         "0.5435047445064762"
        ],
        [
         "30",
         "31",
         "8.375207331019071e-06",
         "1.3036678229021964e-05",
         "0.7269226009339915"
        ],
        [
         "31",
         "32",
         "9.04135588930665e-06",
         "1.3189198428074068e-06",
         "0.9174165184889904"
        ],
        [
         "32",
         "33",
         "1.382364020713247e-06",
         "4.0143583696526055e-05",
         "0.26700246347657997"
        ],
        [
         "33",
         "34",
         "3.948794828869288e-05",
         "8.427763518108998e-06",
         "0.21419344387060024"
        ],
        [
         "34",
         "35",
         "7.778405654242318e-06",
         "3.2680396393254845e-06",
         "0.8196825649268062"
        ],
        [
         "35",
         "36",
         "3.6898879917536253e-06",
         "2.6330099100776976e-05",
         "-0.07556953070116013"
        ],
        [
         "36",
         "37",
         "6.333362688825166e-05",
         "2.0121150825082783e-05",
         "0.4805936186746437"
        ],
        [
         "37",
         "38",
         "2.449207480790175e-05",
         "6.18062207590852e-05",
         "-0.9365793146863959"
        ],
        [
         "38",
         "39",
         "6.306540348854948e-05",
         "1.7370816959447067e-05",
         "0.4197065244483079"
        ],
        [
         "39",
         "40",
         "1.5708161708161983e-05",
         "7.322042676300738e-06",
         "0.40570649815640136"
        ],
        [
         "40",
         "41",
         "7.475367352202244e-06",
         "9.157245926593457e-06",
         "0.5164679577164603"
        ],
        [
         "41",
         "42",
         "1.1066956123297974e-05",
         "2.8473266178393513e-06",
         "0.6780879387321438"
        ],
        [
         "42",
         "43",
         "2.0078994154741085e-06",
         "2.0871935937941523e-06",
         "0.8215335772852521"
        ],
        [
         "43",
         "44",
         "1.93472378307359e-06",
         "3.991534848706535e-06",
         "0.7453771735393968"
        ],
        [
         "44",
         "45",
         "3.1722305349563594e-06",
         "2.299042944412068e-06",
         "0.8119242102545056"
        ],
        [
         "45",
         "46",
         "5.368781142004849e-06",
         "2.4861635151146675e-05",
         "0.008232150799876714"
        ],
        [
         "46",
         "47",
         "2.487652228721649e-05",
         "1.5084229680074674e-05",
         "0.14302761993807767"
        ],
        [
         "47",
         "48",
         "1.492840220318585e-05",
         "2.0086390373193562e-05",
         "0.3544916380061872"
        ],
        [
         "48",
         "49",
         "2.140271488187723e-05",
         "6.9631248333655685e-06",
         "0.6468469744605403"
        ],
        [
         "49",
         "50",
         "7.2097045895829605e-06",
         "5.580077501013645e-05",
         "-0.49822262562052266"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 60
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.883342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.462335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.432679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.071878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.702118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.018483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>-0.009857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>-0.005543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.375038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.070040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.374222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.749349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.780811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.919826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.791916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.552603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.513340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.714036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.092669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.609159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.835679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.411905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.594253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.760576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.754037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.181936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.344425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.047460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.140474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.543505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.726923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.917417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.267002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.214193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.819683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.075570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.480594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.936579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.419707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.405706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.516468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.678088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.821534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.745377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.811924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.008232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.143028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.354492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.646847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>-0.498223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.190556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.705676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.594686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.899861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.576174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.711067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.780460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.927886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.667428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.007730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fold   val_mse  test_mse   test r2\n",
       "0      1  0.000013  0.000002  0.883342\n",
       "1      2  0.000002  0.000008  0.462335\n",
       "2      3  0.000010  0.000041 -0.432679\n",
       "3      4  0.000041  0.000027  0.071878\n",
       "4      5  0.000027  0.000004  0.702118\n",
       "5      6  0.000004  0.000035 -0.018483\n",
       "6      7  0.000035  0.000058 -0.009857\n",
       "7      8  0.000058  0.000109 -0.005543\n",
       "8      9  0.000109  0.000111  0.375038\n",
       "9     10  0.000105  0.000118  0.070040\n",
       "10    11  0.000118  0.000036  0.374222\n",
       "11    12  0.000033  0.000015  0.749349\n",
       "12    13  0.000014  0.000040  0.780811\n",
       "13    14  0.000036  0.000009  0.919826\n",
       "14    15  0.000022  0.000011  0.791916\n",
       "15    16  0.000030  0.000026  0.552603\n",
       "16    17  0.000032  0.000022  0.513340\n",
       "17    18  0.000013  0.000016  0.714036\n",
       "18    19  0.000020  0.000036  0.092669\n",
       "19    20  0.000038  0.000020  0.609159\n",
       "20    21  0.000020  0.000007  0.835679\n",
       "21    22  0.000006  0.000034  0.411905\n",
       "22    23  0.000050  0.000020  0.594253\n",
       "23    24  0.000022  0.000013  0.760576\n",
       "24    25  0.000014  0.000006  0.754037\n",
       "25    26  0.000006  0.000067  0.181936\n",
       "26    27  0.000067  0.000042  0.344425\n",
       "27    28  0.000040  0.000074  0.047460\n",
       "28    29  0.000132  0.000042  0.140474\n",
       "29    30  0.000050  0.000013  0.543505\n",
       "30    31  0.000008  0.000013  0.726923\n",
       "31    32  0.000009  0.000001  0.917417\n",
       "32    33  0.000001  0.000040  0.267002\n",
       "33    34  0.000039  0.000008  0.214193\n",
       "34    35  0.000008  0.000003  0.819683\n",
       "35    36  0.000004  0.000026 -0.075570\n",
       "36    37  0.000063  0.000020  0.480594\n",
       "37    38  0.000024  0.000062 -0.936579\n",
       "38    39  0.000063  0.000017  0.419707\n",
       "39    40  0.000016  0.000007  0.405706\n",
       "40    41  0.000007  0.000009  0.516468\n",
       "41    42  0.000011  0.000003  0.678088\n",
       "42    43  0.000002  0.000002  0.821534\n",
       "43    44  0.000002  0.000004  0.745377\n",
       "44    45  0.000003  0.000002  0.811924\n",
       "45    46  0.000005  0.000025  0.008232\n",
       "46    47  0.000025  0.000015  0.143028\n",
       "47    48  0.000015  0.000020  0.354492\n",
       "48    49  0.000021  0.000007  0.646847\n",
       "49    50  0.000007  0.000056 -0.498223\n",
       "50    51  0.000059  0.000035  0.190556\n",
       "51    52  0.000039  0.000011  0.705676\n",
       "52    53  0.000009  0.000011  0.594686\n",
       "53    54  0.000011  0.000001  0.899861\n",
       "54    55  0.000001  0.000012  0.576174\n",
       "55    56  0.000011  0.000016  0.711067\n",
       "56    57  0.000020  0.000006  0.780460\n",
       "57    58  0.000005  0.000001  0.927886\n",
       "58    59  0.000003  0.000008  0.667428\n",
       "59    60  0.000010  0.000037 -0.007730"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fit the optuna variables later, this is a grid search cell not SOTA\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# -> Param grid\n",
    "params = {\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"n_estimators\": [50, 100, 250, 500],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"subsample\": [0.6, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"n_estimators\": [50, 100, 250, 500]\n",
    "}\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "# -> Initialize results storage\n",
    "results = []\n",
    "all_test_preds = []\n",
    "all_test_idx = []\n",
    "\n",
    "# -> For each fold within the rolling split\n",
    "for fold, (train_idx, val_idx, test_idx) in enumerate(splitter.split(X)):\n",
    "    \"\"\"\n",
    "    Instead of splitting data, we split the indices and use .iloc to get the data\n",
    "    \"\"\"\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val,   y_val   = X.iloc[val_idx],   y.iloc[val_idx]\n",
    "    X_test,  y_test  = X.iloc[test_idx],  y.iloc[test_idx]\n",
    "    \n",
    "    # => Grid search on validation set\n",
    "    best_score = np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for params in grid:\n",
    "        model = XGBRegressor(\n",
    "            objective = 'reg:squarederror',\n",
    "            random_state = 69,\n",
    "            device = 'cuda',\n",
    "            **params\n",
    "        )\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "        val_pred = model.predict(X_val)\n",
    "        val_mse = mean_squared_error(y_val, val_pred)\n",
    "        if val_mse < best_score:\n",
    "            best_score = val_mse\n",
    "            best_params = params\n",
    "\n",
    "    # => Retraining on train/val set\n",
    "    X_train_val = pd.concat([X_train, X_val])\n",
    "    y_train_val = pd.concat([y_train, y_val])\n",
    "\n",
    "    best_model = XGBRegressor(\n",
    "        objective = 'reg:squarederror',\n",
    "        random_state = 69,\n",
    "        device = 'cuda',\n",
    "        **best_params\n",
    "    )\n",
    "\n",
    "    best_model.fit(X_train_val, y_train_val,  verbose=False)\n",
    "    test_preds = best_model.predict(X_test)\n",
    "    test_mse = mean_squared_error(y_test, test_preds)\n",
    "    test_r2 = r2_score(y_test, test_preds)\n",
    "\n",
    "    # => Appending results for all folds -\n",
    "    results.append({\n",
    "    \"fold\": fold + 1,\n",
    "    \"val_mse\": best_score,\n",
    "    \"test_mse\": test_mse,\n",
    "    \"test r2\": test_r2,\n",
    "    })\n",
    "\n",
    "    all_test_preds.extend(test_preds)\n",
    "    all_test_idx.extend(test_idx)\n",
    "\n",
    "# -> Results summary \n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
